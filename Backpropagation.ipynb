{
 "metadata": {
  "name": "",
  "signature": "sha256:bdd1201f7d4b5de2364f567654f09a729c33ff1c35b585c64d1b8ccdbba5868b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Backpropagation\n",
      "\n",
      "Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters.  It may be the most common method for training neural networks.  Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors.  For a somewhat more accessible and readable (but slightly less complete) tutorial which uses notation similar to this one, please see\n",
      "http://neuralnetworksanddeeplearning.com/chap2.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Review: The chain rule\n",
      "\n",
      "The chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another variable $w$, then the chain rule states that\n",
      "$$\n",
      "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
      "$$\n",
      "For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
      "$$\n",
      "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notation\n",
      "\n",
      "In the following derivation, we'll use the following notation:\n",
      "\n",
      "$L$ - Number of layers in the network.\n",
      "\n",
      "$N^n$ - Dimensionality of layer $n \\in \\{0, \\ldots, L\\}$.  $N^0$ is the dimensionality of the input; $N^L$ is the dimensionality of the output.\n",
      "\n",
      "$W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ - Weight matrix for layer $m \\in \\{1, \\ldots, L\\}$.  $W^m_{ij}$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.\n",
      "\n",
      "$b^m \\in \\mathbb{R}^{N^m}$ - Bias vector for layer $m$.\n",
      "\n",
      "$\\sigma^m$ - Nonlinear activation function of the units in layer $m$, applied elementwise.\n",
      "\n",
      "$z^m \\in \\mathbb{R}^{N^m}$ - Linear mix of the inputs to layer $m$, computed by $z^m = W^m a^{m - 1} + b^m$.\n",
      "\n",
      "$a^m \\in \\mathbb{R}^{N^m}$ - Activation of units in layer $m$, computed by $a^m = \\sigma^m(h^m) = \\sigma^m(W^m a^{m - 1} + b^m)$.  $a^L$ is the output of the network.  We define the special case $a^0$ as the input of the network.\n",
      "\n",
      "$y \\in \\mathbb{R}^{N^L}$ - Target output of the network.\n",
      "\n",
      "$C$ - Cost/error function of the network, which is a function of $a^L$ (the network output) and $y$ (treated as a constant)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Backpropagation in general\n",
      "\n",
      "In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the cost/error function $C$; that is, we need to know $\\frac{\\partial C}{\\partial W^m}$ and $\\frac{\\partial C}{\\partial b^m}$.  It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture:\n",
      "\n",
      "- $\\frac{\\partial C}{\\partial a^L}$: The derivative of the cost function with respect to its argument, the output of the network\n",
      "- $\\frac{\\partial a^m}{\\partial z^m}$: The derivative of the nonlinearity used in layer $m$ with respect to its argument\n",
      "\n",
      "To compute the gradient of our cost/error function $C$ to $W^m_{ij}$ (a single entry in the weight matrix of the layer $m$), we can first note that $C$ is a function of $a^L$, which is itself a function of the linear mix variables $z^m_k$, which are themselves functions of the weight matrices $W^m$ and biases $b^m$.  With this in mind, we can use the chain rule as follows:\n",
      "\n",
      "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial W^m_{ij}}$$\n",
      "\n",
      "Note that by definition \n",
      "$$\n",
      "z^m_k = \\sum_{l = 1}^{N^m} W^m_{kl} a_l^{m - 1} + b^m_k\n",
      "$$\n",
      "It follows that $\\frac{\\partial z^m_k}{\\partial W^m_{ij}}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any elements in $W^m$ except for those in the $k$<sup>th</sup> row, and we are only considering the entry $W^m_{ij}$.  When $i = k$, we have\n",
      "\n",
      "\\begin{align*}\n",
      "\\frac{\\partial z^m_i}{\\partial W^m_{ij}} &= \\frac{\\partial}{\\partial W^m_{ij}}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
      "&= a^{m - 1}_j\\\\\n",
      "\\rightarrow \\frac{\\partial z^m_k}{\\partial W^m_{ij}} &= \\begin{cases}\n",
      "0 & k \\ne i\\\\\n",
      "a^{m - 1}_j & k = i\n",
      "\\end{cases}\n",
      "\\end{align*}\n",
      "\n",
      "The fact that $\\frac{\\partial C}{\\partial a^m_k}$ is $0$ unless $k = i$ causes the summation above to collapse, giving\n",
      "\n",
      "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1}_j$$\n",
      "\n",
      "Similarly for the bias variables $b^m$, we have\n",
      "\n",
      "$$\\frac{\\partial C}{\\partial b^m_i} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial b^m_i}$$\n",
      "\n",
      "As above, it follows that $\\frac{\\partial z^m_k}{\\partial b^m_i}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any element in $b^m$ except $b^m_k$.  When $i = k$, we have\n",
      "\n",
      "\\begin{align*}\n",
      "\\frac{\\partial z^m_i}{\\partial b^m_i} &= \\frac{\\partial}{\\partial b^m_i}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
      "&= 1\\\\\n",
      "\\rightarrow \\frac{\\partial z^m_i}{\\partial b^m_i} &= \\begin{cases}\n",
      "0 & k \\ne i\\\\\n",
      "1 & k = i\n",
      "\\end{cases}\n",
      "\\end{align*}\n",
      "\n",
      "The summation also collapses to give\n",
      "\n",
      "$$\\frac{\\partial C}{\\partial b^m_i} = \\frac{\\partial C}{\\partial z^m_i}$$\n",
      "\n",
      "Now, we must compute $\\frac{\\partial C}{\\partial z^m_k}$.  For the final layer ($m = L$), this term is straightforward to compute using the chain rule:\n",
      "\n",
      "$$\n",
      "\\frac{\\partial C}{\\partial z^L_k} = \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_k}\n",
      "$$\n",
      "\n",
      "or, in vector form\n",
      "\n",
      "$$\n",
      "\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\n",
      "$$\n",
      "\n",
      "The first term $\\frac{\\partial C}{\\partial a^L}$ is just the derivative of the cost function with respect to its argument, whose form depends on the cost function chosen.  Similarly, $\\frac{\\partial a^m}{\\partial z^m}$ (for any layer $m$ includling $L$) is the derivative of the layer's nonlinearity with respect to its argument and will depend on the choice of nonlinearity.  For other layers, we again invoke the chain rule:\n",
      "\n",
      "\n",
      "\\begin{align*}\n",
      "\\frac{\\partial C}{\\partial z^m_k} &= \\frac{\\partial C}{\\partial a^m_k} \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
      "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial z^{m + 1}_l}{\\partial a^m_k}\\right)\\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
      "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial}{\\partial a^m_k} \\left(\\sum_{h = 1}^{N^m} W^{m + 1}_{lh} a_h^m + b_l^{m + 1}\\right)\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
      "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l} W^{m + 1}_{lk}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
      "&= \\left(\\sum_{l = 1}^{N^{m + 1}}W^{m + 1\\top}_{kl} \\frac{\\partial C}{\\partial z^{m + 1}_l}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
      "\\end{align*}\n",
      "\n",
      "where the last simplification was made because by convention $\\frac{\\partial C}{\\partial z^{m + 1}_l}$ is a column vector, allowing us to write the following vector form:\n",
      "\n",
      "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
      "\n",
      "Note that we now have the ingredients to efficiently compute the gradient of the cost function with respect to the network's parameters:  First, we compute $\\frac{\\partial C}{\\partial z^L_k}$ based on the choice of cost function and nonlinearity.  Then, we recursively can compute $\\frac{\\partial C}{\\partial z^m}$ layer-by-layer based on the term $\\frac{\\partial C}{\\partial z^{m + 1}}$ computed from the previous layer and the nonlinearity of the layer (this is called the \"backward pass\")."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Backpropagation in practice\n",
      "\n",
      "As discussed above, the exact form of the updates depends on both the chosen cost function and each layer's chosen nonlinearity.  The following two tables list the needed partial derivatives for some common choices of each.\n",
      "\n",
      "| Nonlinearity | $a^m = \\sigma^m(z^m)$ | $\\frac{\\partial a^m}{\\partial z^m}$ |\n",
      "|--------------|---|---|\n",
      "| Sigmoid      | $\\frac{1}{1 + e^{z^m}}$ | $\\sigma^m(z^m)(1 - \\sigma^m(z^m) = a^m(1 - a^m)$ |\n",
      "| Tanh         | $\\frac{e^{z^m} - e^{-z^m}}{e^{z^m} + e^{-z^m}}$ | $\\sigma^m(z^m)(1 - (\\sigma^m(z^m))^2) = a^m(1 - (a^m)^2)$ |\n",
      "| ReLU         | $\\max(0, z^m)$ | $0, z^m < 0;\\; 1, z^m \\ge 0$ |\n",
      "\n",
      "\n",
      "| Cost Function | $C$                                  | $\\frac{\\partial C}{\\partial a^L}$ |\n",
      "|---------------|--------------------------------------|-----------------------------------|\n",
      "| Squared Error | $\\frac{1}{2}(y - a^L)^\\top(y - a^L)$ | $y - a^L$                         |\n",
      "| Cross-Entropy | $(y - 1)\\log(1 - a^L) - y\\log(a^L)$  | $\\frac{a^L - y}{a^L(1 - a^L)}$    |"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "np.random.seed(0)\n",
      "\n",
      "def tanh(x):\n",
      "    return np.tanh(x)\n",
      "\n",
      "def tanh_deriv(x):\n",
      "    return 1.0 - x**2\n",
      "\n",
      "def logistic(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def logistic_derivative(x):\n",
      "    return logistic(x)*(1-logistic(x))\n",
      "\n",
      "class NeuralNetwork:\n",
      "\n",
      "    def __init__(self, layers, activation='tanh'):\n",
      "        \"\"\"\n",
      "        :param layers: A list containing the number of units in each layer. Should be at least two values\n",
      "        :param activation: The activation function to be used. Can be \"logistic\" or \"tanh\"\n",
      "        \"\"\"\n",
      "        if activation == 'logistic':\n",
      "            self.activation = logistic\n",
      "            self.activation_deriv = logistic_derivative\n",
      "        elif activation == 'tanh':\n",
      "            self.activation = tanh\n",
      "            self.activation_deriv = tanh_deriv\n",
      "\n",
      "        self.weights = []\n",
      "        for i in range(1, len(layers) - 1):\n",
      "            self.weights.append((2*np.random.random((layers[i - 1], layers[i]))-1)*0.25)\n",
      "        self.weights.append((2*np.random.random((layers[i], layers[i + 1]))-1)*0.25)\n",
      "        \n",
      "    def fit(self, X, y, learning_rate=0.2, epochs=10000):\n",
      "        X = np.atleast_2d(X)\n",
      "        #temp = np.ones([X.shape[0], X.shape[1]+1])\n",
      "        #temp[:, 0:-1] = X  # adding the bias unit to the input layer\n",
      "        #X = temp\n",
      "        y = np.array(y)\n",
      "\n",
      "        for k in range(epochs):\n",
      "            i = np.random.randint(X.shape[0])\n",
      "            a = [X[i]]\n",
      "\n",
      "            for l in range(len(self.weights)):\n",
      "                    a.append(self.activation(np.dot(a[l], self.weights[l])))\n",
      "            error = y[i] - a[-1]\n",
      "            deltas = [error * self.activation_deriv(a[-1])]\n",
      "\n",
      "            for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer\n",
      "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))\n",
      "            deltas.reverse()\n",
      "            for i in range(len(self.weights)):\n",
      "                layer = np.atleast_2d(a[i])\n",
      "                delta = np.atleast_2d(deltas[i])\n",
      "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
      "\n",
      "    def predict(self, x):\n",
      "        x = np.array(x)\n",
      "        #temp = np.ones(x.shape[0]+1)\n",
      "        #temp[0:-1] = x\n",
      "        a = x#temp\n",
      "        for l in range(0, len(self.weights)):\n",
      "            a = self.activation(np.dot(a, self.weights[l]))\n",
      "        return a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NeuralNetwork([2,2,1], 'tanh')\n",
      "X = np.array([[0, 0],\n",
      "              [0, 1],\n",
      "              [1, 0],\n",
      "              [1, 1]])\n",
      "y = np.array([1, 0, 1, 0])\n",
      "nn.fit(X, y)\n",
      "for i in [[0, 0], [0, 1], [1, 0], [1, 1]]:\n",
      "    print i, nn.predict(i)#1*(nn.predict(i) > .5).flatten()[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0] [ 0.]\n",
        "[0, 1] [-0.00941255]\n",
        "[1, 0] [ 0.97730902]\n",
        "[1, 1] [ 0.02821457]\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "np.random.seed(0)\n",
      "\n",
      "def tanh(x):\n",
      "    return np.tanh(x)\n",
      "\n",
      "def tanh_deriv(x):\n",
      "    return 1.0 - x**2\n",
      "\n",
      "def logistic(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def logistic_derivative(x):\n",
      "    return logistic(x)*(1-logistic(x))\n",
      "\n",
      "class NeuralNetwork:\n",
      "\n",
      "    def __init__(self, layers, activation='tanh'):\n",
      "        \"\"\"\n",
      "        :param layers: A list containing the number of units in each layer. Should be at least two values\n",
      "        :param activation: The activation function to be used. Can be \"logistic\" or \"tanh\"\n",
      "        \"\"\"\n",
      "        if activation == 'logistic':\n",
      "            self.activation = logistic\n",
      "            self.activation_deriv = logistic_derivative\n",
      "        elif activation == 'tanh':\n",
      "            self.activation = tanh\n",
      "            self.activation_deriv = tanh_deriv\n",
      "\n",
      "        self.weights = []\n",
      "        self.biases = []\n",
      "        for i in range(1, len(layers)):\n",
      "            self.weights.append(.5*np.random.random((layers[i], layers[i - 1])) - 0.25)\n",
      "            self.biases.append(.5*np.random.random((layers[i], 1)) - 0.25)\n",
      "        \n",
      "    def train(self, X, z, learning_rate=0.2):\n",
      "        # Convert arrays to column vectors\n",
      "        if X.ndim == 1:\n",
      "            X = X.reshape(-1, 1)\n",
      "        if z.ndim == 1:\n",
      "            z = z.reshape(-1, 1)\n",
      "            \n",
      "        a = [self.activation(np.dot(self.weights[0], X))]# + self.biases[0]]\n",
      "\n",
      "        # Forward pass\n",
      "        for m in xrange(1, len(self.weights)):\n",
      "            # a^m = sigma^m(W^m a^(m - 1) + b^m)\n",
      "            a.append(self.activation(np.dot(self.weights[m], a[m - 1])))# + self.biases[m])\n",
      "        # \\delta^L = (z - a^L) o a^L o (1 - a^L)\n",
      "        deltas = [(z - a[-1])*self.activation_deriv(a[-1])]\n",
      "\n",
      "        # For layers from L - 1 to 1\n",
      "        for m in reversed(xrange(len(a) - 1)):\n",
      "            # (W^{(m + 1)T} \\delta^{m + 1}) o a^m o (1 - a^m)\n",
      "            deltas.append(np.dot(self.weights[m + 1].T, deltas[-1])*self.activation_deriv(a[m]))\n",
      "        deltas.reverse()\n",
      "        #print [b.shape for b in a] \n",
      "        #print [w.shape for w in self.weights]\n",
      "        #print [d.shape for d in deltas]\n",
      "        self.weights[0] += learning_rate*np.dot(deltas[0], X.T)\n",
      "        for m in range(1, len(self.weights)):\n",
      "            self.weights[m] += learning_rate*np.dot(deltas[m], a[m - 1].T)\n",
      "            #self.biases[m] += learning_rate*deltas[m]\n",
      "\n",
      "    def predict(self, X):\n",
      "        if X.ndim == 1:\n",
      "            X = X.reshape(-1, 1)\n",
      "        a = X\n",
      "        for m in xrange(len(self.weights)):\n",
      "            a = self.activation(np.dot(self.weights[m], a))# + self.biases[m]\n",
      "        return a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NeuralNetwork([2, 3, 4, 7, 1], 'tanh')\n",
      "X = np.array([[0, 0, 1, 1],\n",
      "              [0, 1, 0, 1]])\n",
      "z = np.array([[0, 1, 1, 0]])\n",
      "for n in xrange(10000):\n",
      "    i = np.random.choice(X.shape[1])\n",
      "    nn.train(X[:, i], z[:, i])\n",
      "print X\n",
      "print 1*(nn.predict(X) > .5)\n",
      "print nn.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 1 1]\n",
        " [0 1 0 1]]\n",
        "[[0 1 1 0]]\n",
        "[[ 0.          0.99299805  0.99071787 -0.02635698]]\n"
       ]
      }
     ],
     "prompt_number": 219
    }
   ],
   "metadata": {}
  }
 ]
}